{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import generate as generate\n",
    "from single_layer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "- Data Generation: R1 -> R1, with only one activation unit\n",
    "- model \n",
    "    - hidden_dim: [20]\n",
    "    - lr: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "d = 8\n",
    "M = 4\n",
    "num = 1\n",
    "T = 32\n",
    "N_test = 102400\n",
    "noise = 0.1\n",
    "\n",
    "lr = 0.01\n",
    "hidden_dim = [32, 32, 32]\n",
    "dropout = 0.5\n",
    "weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 8)\n",
      "(1, 32)\n"
     ]
    }
   ],
   "source": [
    "(thetan, an, bn) = generate.generate_single_layer_v2(M, d, num)\n",
    "(X, Y_noiseless) = generate.generate_single_data_v2(T, an, bn, thetan)\n",
    "Y = generate.add_noise(Y_noiseless, noise)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 64\n",
    "hidden_dim = [64, 64, 64]\n",
    "for T in [8192]:\n",
    "    for _ in range(num_experiments):\n",
    "        (thetan, an, bn) = generate.generate_single_layer_v2(M, d, num)\n",
    "        (X, Y_noiseless) = generate.generate_single_data_v2(T, an, bn, thetan)\n",
    "        Y = generate.add_noise(Y_noiseless, noise)\n",
    "        input = X[0]\n",
    "        (model, epoch_number, best_vloss, train_loss) = train_one_model(\n",
    "            hidden_dim, X[0], Y[0], \n",
    "            val_ratio=0.2, \n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay,\n",
    "            dropout=dropout,\n",
    "            batch_size=4096,\n",
    "            patience=20, \n",
    "            epochs=500,\n",
    "            verbose=False,\n",
    "        )\n",
    "        # print(f\"epochs: {epoch_number}, train loss: {train_loss} validation loss: {best_vloss}\")\n",
    "        # plt.plot(*zip(*sorted(zip(X[0], predicted))))\n",
    "        # plt.legend([\"real\"] + [\"predicted\"])\n",
    "        # plt.show()\n",
    "        model.eval()\n",
    "        (X_test, Y_test) = generate.generate_single_data_v2(N_test, an, bn, thetan)\n",
    "        predicted = model(torch.Tensor(X_test)).detach().numpy()\n",
    "        kl_divergence = generate.kl_divergence(Y_test, predicted.reshape(-1), noise)\n",
    "        print(f\"{d}\\t{M}\\t{T}\\t{noise}\\t{kl_divergence[0]}\\t{hidden_dim}\\t{dropout}\\t{weight_decay}\")\n",
    "        # print(generate.kl_divergence(Y_test, 0, noise))\n",
    "        # print(kl_divergence / generate.kl_divergence(Y_test, 0, noise))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ok\":true,\"result\":{\"message_id\":208,\"from\":{\"id\":883542720,\"is_bot\":true,\"first_name\":\"\\u5c0f\\u52a9\\u624b\",\"username\":\"MMMyNotificationBot\"},\"chat\":{\"id\":1172039795,\"first_name\":\"\\u534a\\u4ed9\",\"type\":\"private\"},\"date\":1649110942,\"text\":\"training finished\"}}"
     ]
    }
   ],
   "source": [
    "!notify.sh 'training finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 524288, 8) (1, 524288)\n"
     ]
    }
   ],
   "source": [
    "T = 2**19\n",
    "M = 4\n",
    "d = 8\n",
    "(thetan, an, bn) = generate.generate_single_layer_v2(M, d, num)\n",
    "(X, Y_noiseless) = generate.generate_single_data_v2(T, an, bn, thetan)\n",
    "Y = generate.add_noise(Y_noiseless, noise)\n",
    "input = X[0]\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.5092933320417636 valid 0.1130468025803566\n",
      "EPOCH 2:\n",
      "LOSS train 0.27473637258134237 valid 0.10431107133626938\n",
      "EPOCH 3:\n",
      "LOSS train 0.2522524456788854 valid 0.1119113564491272\n",
      "EPOCH 4:\n",
      "LOSS train 0.2317112789648335 valid 0.07538247108459473\n",
      "EPOCH 5:\n",
      "LOSS train 0.21920956682141235 valid 0.08208274841308594\n",
      "EPOCH 6:\n",
      "LOSS train 0.21737110374904262 valid 0.08745074272155762\n",
      "EPOCH 7:\n",
      "LOSS train 0.21492387212631178 valid 0.0676506906747818\n",
      "EPOCH 8:\n",
      "LOSS train 0.21028356359499256 valid 0.07523845136165619\n",
      "EPOCH 9:\n",
      "LOSS train 0.21345358123866523 valid 0.08272461593151093\n",
      "EPOCH 10:\n",
      "LOSS train 0.21048184215295607 valid 0.062370415776968\n",
      "EPOCH 11:\n",
      "LOSS train 0.20967752849910318 valid 0.05488964170217514\n",
      "EPOCH 12:\n",
      "LOSS train 0.2138896323922204 valid 0.06744629889726639\n",
      "EPOCH 13:\n",
      "LOSS train 0.21120101111691172 valid 0.07002749294042587\n",
      "EPOCH 14:\n",
      "LOSS train 0.21120364302542152 valid 0.07936234027147293\n",
      "EPOCH 15:\n",
      "LOSS train 0.20866841095976713 valid 0.06818018853664398\n",
      "EPOCH 16:\n",
      "LOSS train 0.21034901156658078 valid 0.06397046893835068\n",
      "EPOCH 17:\n",
      "LOSS train 0.21238258484660125 valid 0.07624591141939163\n",
      "EPOCH 18:\n",
      "LOSS train 0.20740602652474147 valid 0.05511659383773804\n",
      "EPOCH 19:\n",
      "LOSS train 0.21220407049830367 valid 0.06625552475452423\n",
      "EPOCH 20:\n",
      "LOSS train 0.20964247849656314 valid 0.07790836691856384\n",
      "EPOCH 21:\n",
      "LOSS train 0.2092528033910728 valid 0.07027385383844376\n",
      "EPOCH 22:\n",
      "LOSS train 0.2109847843647003 valid 0.07608075439929962\n",
      "EPOCH 23:\n",
      "LOSS train 0.2098635064392555 valid 0.0677720233798027\n",
      "EPOCH 24:\n",
      "LOSS train 0.2110257881080232 valid 0.06091490387916565\n",
      "EPOCH 25:\n",
      "LOSS train 0.21000462090823707 valid 0.06691382080316544\n",
      "EPOCH 26:\n",
      "LOSS train 0.21071033594084948 valid 0.07312030345201492\n",
      "EPOCH 27:\n",
      "LOSS train 0.2110384010687107 valid 0.07256165146827698\n",
      "EPOCH 28:\n",
      "LOSS train 0.2094805632786053 valid 0.06969621032476425\n",
      "EPOCH 29:\n",
      "LOSS train 0.21233977152080072 valid 0.0698937177658081\n",
      "EPOCH 30:\n",
      "LOSS train 0.21028223441141408 valid 0.07166779041290283\n",
      "EPOCH 31:\n",
      "LOSS train 0.21216277829030664 valid 0.06989265978336334\n",
      "EPOCH 32:\n",
      "LOSS train 0.20957149176335915 valid 0.0762716606259346\n",
      "EPOCH 33:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (model, epoch_number, best_vloss, train_loss) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(f\"epochs: {epoch_number}, train loss: {train_loss} validation loss: {best_vloss}\")\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# plt.plot(*zip(*sorted(zip(X[0], predicted))))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# plt.legend([\"real\"] + [\"predicted\"])\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/windows/Users/fanzh/Desktop/zhuyifan/Stanford/Activities/2022/RL-Ben/code/single_layer.py:169\u001b[0m, in \u001b[0;36mtrain_one_model\u001b[0;34m(hidden_dim, x, y, val_ratio, lr, weight_decay, dropout, batch_size, model, **train_kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    168\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m--> 169\u001b[0m (epoch_number, best_vloss, avg_loss) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_early_stopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (model, epoch_number, best_vloss, avg_loss)\n",
      "File \u001b[0;32m/windows/Users/fanzh/Desktop/zhuyifan/Stanford/Activities/2022/RL-Ben/code/single_layer.py:117\u001b[0m, in \u001b[0;36mtrain_early_stopping\u001b[0;34m(model, optimizer, loss_fn, training_loader, validation_loader, epochs, patience, verbose)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m    116\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 117\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# We don't need gradients on to do reporting\u001b[39;00m\n\u001b[1;32m    120\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/windows/Users/fanzh/Desktop/zhuyifan/Stanford/Activities/2022/RL-Ben/code/single_layer.py:76\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, loss_fn, training_loader)\u001b[0m\n\u001b[1;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     78\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n",
      "File \u001b[0;32m~/.conda/envs/rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/windows/Users/fanzh/Desktop/zhuyifan/Stanford/Activities/2022/RL-Ben/code/single_layer.py:42\u001b[0m, in \u001b[0;36mMultiLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfcs[\u001b[38;5;241m0\u001b[39m](x)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_hidden):\n\u001b[0;32m---> 42\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfcs[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/rl/lib/python3.9/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rl/lib/python3.9/site-packages/torch/nn/functional.py:1169\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[0;32m-> 1169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(model, epoch_number, best_vloss, train_loss) = train_one_model(\n",
    "    [128, 128, 128], X[0], Y[0], \n",
    "    val_ratio=0.2, \n",
    "    lr=lr, \n",
    "    weight_decay=0.001,\n",
    "    dropout=0.5,\n",
    "    batch_size=1024,\n",
    "    patience=40, \n",
    "    epochs=1000,\n",
    "    verbose=True,\n",
    ")\n",
    "# print(f\"epochs: {epoch_number}, train loss: {train_loss} validation loss: {best_vloss}\")\n",
    "# plt.plot(*zip(*sorted(zip(X[0], predicted))))\n",
    "# plt.legend([\"real\"] + [\"predicted\"])\n",
    "# plt.show()\n",
    "model.eval()\n",
    "(X_test, Y_test) = generate.generate_single_data_v2(N_test, an, bn, thetan)\n",
    "predicted = model(torch.Tensor(X_test)).detach().numpy()\n",
    "kl_divergence = generate.kl_divergence(predicted.reshape(-1), Y_test, noise)\n",
    "print(kl_divergence[0])\n",
    "# print(generate.kl_divergence(Y_test, 0, noise))\n",
    "# print(kl_divergence / generate.kl_divergence(Y_test, 0, noise))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayer(\n",
       "  (fcs): ModuleList(\n",
       "    (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34305"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6 / "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "381d4131ad07596e9720955678587090e535b4a843f544edaede8100ef2767e0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
