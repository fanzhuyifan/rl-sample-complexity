""" Fitting data generated by a single layer generation process
"""

from copy import deepcopy
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from torch.utils.tensorboard import SummaryWriter

class SingleLayer(nn.Module):
    """ Single Layer Model
    """
    def __init__(self, d, hidden_size, act=nn.LeakyReLU()):
        super(SingleLayer, self).__init__()
        self.fc1 = nn.Linear(d, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)
        self.act = act

    def forward(self, x):
        x = self.act(self.fc1(x))
        x = self.fc2(x)
        return x

class MultiLayer(nn.Module):
    """ Multi Layer Model
    """
    def __init__(self, d, hidden_size, act=nn.LeakyReLU(), dropout=0, batchNorm=False):
        super(MultiLayer, self).__init__()
        self.n_hidden = len(hidden_size)
        self.fcs = nn.ModuleList()
        self.fcs.append(nn.Linear(d, hidden_size[0]))
        for i in range(self.n_hidden - 1):
            self.fcs.append(nn.Linear(hidden_size[i], hidden_size[i+1]))
        self.fcs.append(nn.Linear(hidden_size[-1], 1))
        self.dropout = nn.Dropout(dropout)
        self.act = act
        self.batchNorm = batchNorm
        if self.batchNorm:
            self.batch_norms = nn.ModuleList()
            for hidden_size in hidden_size:
                self.batch_norms.append(nn.BatchNorm1d(hidden_size))


    def forward(self, x):
        x = self.fcs[0](x)
        for i in range(self.n_hidden):
            x = self.dropout(x)
            if self.batchNorm:
                x = self.batch_norms[i](x)
            x = self.act(x)
            x = self.fcs[i+1](x)
        return x

def get_data_loader(x, y, batch_size=100):
    """ Returns data_loader from numpy arrays.
    """
    tensor_x = torch.Tensor(x)
    tensor_y = torch.Tensor(y[..., np.newaxis])
    dataset = TensorDataset(tensor_x,tensor_y)
    dataloader = DataLoader(dataset, batch_size=batch_size)
    return dataloader

def train_one_epoch(
    model,
    optimizer,
    loss_fn,
    training_loader, 
):
    """
    :return: average trainign loss
    """
    running_loss = 0.
    # Here, we use enumerate(training_loader) instead of
    # iter(training_loader) so that we can track the batch
    # index and do some intra-epoch reporting
    for i, data in enumerate(training_loader):
        # Every data instance is an input + label pair
        inputs, labels = data

        # Zero your gradients for every batch!
        optimizer.zero_grad(set_to_none=True)

        # Make predictions for this batch
        outputs = model(inputs)
        # Compute the loss and its gradients
        loss = loss_fn(outputs, labels)
        loss.backward()

        # Adjust learning weights
        optimizer.step()

        # Gather data and report
        running_loss += loss.item()

    return running_loss / (i+1)


def train_early_stopping(
    model,
    optimizer,
    loss_fn,
    training_loader, 
    validation_loader, 
    epochs = 1000,
    patience = 5,
    verbose = False,
):
    """ 
    :param patience: Number of epochs with no improvement after which training will be stopped
    """
    from datetime import datetime
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    if verbose:
        writer = SummaryWriter('runs/{}'.format(timestamp))

    best_vloss = np.inf
    early_stopping = 0

    for epoch in range(epochs):
        if verbose:
            print('EPOCH {}:'.format(epoch + 1))

        # Make sure gradient tracking is on, and do a pass over the data
        model.train(True)
        avg_loss = train_one_epoch(model, optimizer, loss_fn, training_loader)

        # We don't need gradients on to do reporting
        model.train(False)

        running_vloss = 0.0
        for i, vdata in enumerate(validation_loader):
            vinputs, vlabels = vdata
            voutputs = model(vinputs)
            vloss = loss_fn(voutputs, vlabels)
            running_vloss += vloss

        avg_vloss = running_vloss / (i + 1)

        if verbose:
            print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))
            writer.add_scalars(
                'LOSS',
                { 'Training' : avg_loss, 'Validation' : avg_vloss },
                epoch + 1
            )
            writer.flush()
        if avg_vloss >= best_vloss:
            early_stopping += 1
        else:
            best_vloss = avg_vloss
            early_stopping = 0
            best_model_state = deepcopy(model.state_dict())
        if early_stopping >= patience:
            break
    model.load_state_dict(best_model_state)

    return (epoch + 1, best_vloss, avg_loss)

def train_one_model(
    hidden_dim, x, y, 
    val_ratio=0.2,
    lr = 0.001,
    weight_decay = 0.01,
    dropout = 0,
    batch_size = 128,
    model = None,
    act = nn.LeakyReLU(),
    batchNorm = False,
    **train_kwargs,
):
    """
    """
    (T, d) = x.shape
    val_size = int(T * val_ratio)
    if model is None:
        model = MultiLayer(d, hidden_dim, dropout=dropout, act=act, batchNorm=batchNorm)
    training_loader = get_data_loader(x[:-val_size], y[:-val_size], batch_size)
    validation_loader = get_data_loader(x[-val_size:], y[-val_size:], batch_size)
    loss_fn = torch.nn.MSELoss(reduction='mean')
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    (epoch_number, best_vloss, avg_loss) = train_early_stopping(
        model,
        optimizer,
        loss_fn,
        training_loader, 
        validation_loader, 
        **train_kwargs,
    )
    return (model, epoch_number, best_vloss, avg_loss)