""" Fitting data generated by a single layer generation process
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader

class SingleLayer(nn.Module):
    """ Single Layer Model
    """
    def __init__(self, d, hidden_size, act=F.relu):
        super(SingleLayer, self).__init__()
        self.fc1 = nn.Linear(d, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)
        self.act = act

    def forward(self, x):
        x = self.act(self.fc1(x))
        x = self.fc2(x)
        return x

def get_data_loader(x, y, batch_size=100):
    """ Returns data_loader from numpy arrays.
    """
    tensor_x = torch.Tensor(x)
    tensor_y = torch.Tensor(y[..., np.newaxis])
    dataset = TensorDataset(tensor_x,tensor_y)
    dataloader = DataLoader(dataset, batch_size=batch_size)
    return dataloader

def train_one_epoch(
    model,
    optimizer,
    loss_fn,
    training_loader, 
):
    """
    :return: average trainign loss
    """
    running_loss = 0.
    # Here, we use enumerate(training_loader) instead of
    # iter(training_loader) so that we can track the batch
    # index and do some intra-epoch reporting
    for i, data in enumerate(training_loader):
        # Every data instance is an input + label pair
        inputs, labels = data

        # Zero your gradients for every batch!
        optimizer.zero_grad()

        # Make predictions for this batch
        outputs = model(inputs)
        # Compute the loss and its gradients
        loss = loss_fn(outputs, labels)
        loss.backward()

        # Adjust learning weights
        optimizer.step()

        # Gather data and report
        running_loss += loss.item()

    return running_loss / (i+1)


def train_early_stopping(
    model,
    optimizer,
    loss_fn,
    training_loader, 
    validation_loader, 
    epochs = 1000,
    patience = 5,
):
    """ 
    :param patience: Number of epochs with no improvement after which training will be stopped
    """
    epoch_number = 0

    best_vloss = np.inf
    early_stopping = 0

    for epoch in range(epochs):
        # print('EPOCH {}:'.format(epoch_number + 1))

        # Make sure gradient tracking is on, and do a pass over the data
        model.train(True)
        avg_loss = train_one_epoch(model, optimizer, loss_fn, training_loader)

        # We don't need gradients on to do reporting
        model.train(False)

        running_vloss = 0.0
        for i, vdata in enumerate(validation_loader):
            vinputs, vlabels = vdata
            voutputs = model(vinputs)
            vloss = loss_fn(voutputs, vlabels)
            running_vloss += vloss

        avg_vloss = running_vloss / (i + 1)
        # print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))

        # Track best performance, and save the model's state
        if avg_vloss >= best_vloss:
            early_stopping += 1
        else:
            best_vloss = avg_vloss
            early_stopping = 0
        if early_stopping >= patience:
            break

        epoch_number += 1
    return (epoch_number, best_vloss)

def train_one_model(
    hidden_dim, x, y, 
    val_ratio=0.2,
    lr = 0.001,
    **train_kwargs,
):
    """
    """
    (T, d) = x.shape
    val_size = int(T * val_ratio)
    training_loader = get_data_loader(x[:-val_size], y[:-val_size])
    validation_loader = get_data_loader(x[-val_size:], y[-val_size:])
    model = SingleLayer(d, hidden_dim)
    loss_fn = torch.nn.MSELoss(reduction='mean')
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    (epoch_number, best_vloss) = train_early_stopping(
        model,
        optimizer,
        loss_fn,
        training_loader, 
        validation_loader, 
        **train_kwargs,
    )
    return (model, epoch_number, best_vloss)